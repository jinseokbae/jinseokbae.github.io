---
---

@inproceedings {10.2312:egs.20211023,
  booktitle = {Eurographics 2021 - Short Papers},
  editor = {Theisel, Holger and Wimmer, Michael},
  title = {Auto-rigging 3D Bipedal Characters in Arbitrary Poses},
  author = {Kim, Jeonghwan and Son, Hyeontae and Bae, Jinseok and Kim, Young Min},
  year = {2021},
  publisher = {The Eurographics Association},
  ISSN = {1017-4656},
  ISBN = {978-3-03868-133-5},
  DOI = {10.2312/egs.20211023},
  abstract = {We present an end-to-end algorithm that can automatically rig a given 3D character such that it is ready for 3D animation. The animation of a virtual character requires the skeletal motion defined with bones and joints, and the corresponding deformation of the mesh represented with skin weights. While the conventional animation pipeline requires the initial 3D character to be in the predefined default pose, our pipeline can rig a 3D character in arbitrary pose. We handle the increased ambiguity by fixing the skeletal topology and solving for the full deformation space. After the skeletal positions and orientations are fully discovered, we can deform the provided 3D character into the default pose, from which we can animate the character with the help of recent motion-retargeting techniques. Our results show that we can successfully animate initially deformed characters, which was not possible with previous works.},
  bibtex_show={true},
	paper = {https://diglib.eg.org/handle/10.2312/egs20211023},
  video = {https://www.youtube.com/watch?v=1UVNbxYLkE8},
  code = {https://github.com/whitealex95/autorigging-bipedal}
}

@article{min2021gatsbi,
  title={Gatsbi: Generative agent-centric spatio-temporal object interaction},
  author={Min, Cheol-Hui and Bae, Jinseok and Lee, Junho and Kim, Young Min},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3074--3083},
  year={2021},
  abstract = {We present GATSBI, a generative model that can transform a sequence of raw observations into a structured latent representation that fully captures the spatio-temporal context of the agent's actions. In vision-based decision-making scenarios, an agent faces complex high-dimensional observations where multiple entities interact with each other. The agent requires a good scene representation of the visual observation that discerns essential components and consistently propagates along the time horizon. Our method, GATSBI, utilizes unsupervised object-centric scene representation learning to separate an active agent, static background, and passive objects. GATSBI then models the interactions reflecting the causal relationships among decomposed entities and predicts physically plausible future states. Our model generalizes to a variety of environments where different types of robots and objects dynamically interact with each other. We show GATSBI achieves superior performance on scene decomposition and video prediction compared to its state-of-the-art counterparts.},
	bibtex_show={true},
	paper = {https://arxiv.org/abs/2104.04275},
  video = {https://www.youtube.com/watch?v=nAf87_0T5CE},
}